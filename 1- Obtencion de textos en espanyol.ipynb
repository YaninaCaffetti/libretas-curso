{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/cabecera.png\" width=\"900\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Obtención de textos en español (libreta de curso)\n",
    "\n",
    "## Curso Procesamiento de Lenguaje Natural \n",
    "\n",
    "### Maestría en Tecnologías de la información\n",
    "\n",
    "\n",
    "\n",
    "#### Julio Waissman Vilanova (julio.waissman@unison.mx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los problemas mas importantes a los que se enfrenta uno al desarrollar y aplicar técnicas para el Procesamiento de Lenguaje Natural (PLN) en español, es la falta de *corpus* disponibles que sirvan para la prueba y comparación de diferentes algoritmos. Por otra parte, dado que muchos de los algoritmo desarrollados para el PLN tienen interés al ser utilizados para analizar datos provenientes de redes sociales, es importante poder generar *corpus* provenientes de éstas a partir de la API ofrecida por las diferentes organizaciones y/o compañías.\n",
    "\n",
    "Si bien esta no es una tarea propiamente de PLN, las técnicas modernas de PN requieren de datos de entrenamiento para poder realizar diferentes tareas. En esta libreta se muestran algunas técnicas sencillas para la obtención de diferentes *corpus* y su tratamiento básico en python, utilizando varias herramientas. Cabe resaltar que para aplicaciones más sofisticadas de PLN es necesario contar con *corpus* anotados (esto es, con información extra sobre temas, sentimientos y/o actitudes). Este tipo de *corpus* es dificil de obtener y generalmente se encuentran disponibles en venta, o son generados por las empresas para su utilización particular. Más tarde en en curso hablaremos sobre esto. \n",
    "\n",
    "## 1.1. Algunos *corpus* en español para descarga\n",
    "\n",
    "Estos *corpus* (y su dirección web) pueden ser descargados directamente\n",
    "\n",
    "1. [**Wikimedia dumps data en español**](https://dumps.wikimedia.org/eswiki/20180701/). Es la colección de datos \n",
    "   de *wikipedia*. La información viene en bruto, utilizando archivos en formato xml. Para más información sobre el\n",
    "   formato de los archivos ver https://www.mediawiki.org/xml/export-0.8.xsd (en *Firefox* o *Chrome* de preferencia\n",
    "   para poder navegar en la estructura). El conjunto completo de artículos se encuentran en\n",
    "   https://dumps.wikimedia.org/eswiki/20180701/eswiki-20180701-pages-articles-multistream.xml.bz2 (3 Gb aprox.). \n",
    "   Utilizar de preferencia `curl` o `wget` dado el tamaño del archivo.\n",
    "   \n",
    "2. [**Wikipedia: servicio de exportación**](https://es.wikipedia.org/wiki/Especial:Exportar). En esta direccióon se\n",
    "   encuentra una herramienta de exportación de Wikipedia para exportar entradas particulares. Es posible señalar\n",
    "   categorías y la herramienta carga automáticamente las páginas asociadas a dicha categoría. Esta herramienta solo\n",
    "   funciona con el nombre exacto de la categoría (o subcategoría). Un buen lugar para explorar las categorías de \n",
    "   *Wikipedia* es en su [página principal de portales](https://es.wikipedia.org/wiki/Portal:Portada). En particular, \n",
    "   la herramienta es importante para extraer información de categorás explicitas para entrenar a los métodos en \n",
    "   forma orientada.   \n",
    "\n",
    "3. [**Spanish Billion Word Corpus (SBWC)**](https://crscardellino.github.io/SBWCE/). *Corpus* de documentos \n",
    "   recopilados y agregados por [Cristian Cardellino](https://crscardellino.github.io) de la Universidad Nacional de   \n",
    "   Córdoba. Es una recopilación de documentos de diferentes fuentes disponibles gratuitamente en la web. El documento\n",
    "   comprimido con el *corpus* completo se puede descargar [aqui](http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/clean_corpus.tar.bz2) (2 Gb aprox.). Utilizar de preferencia `curl` o `wget` dado el tamaño del archivo.\n",
    "\n",
    "4. [**Open subtitles**](http://opus.nlpl.eu/OpenSubtitles.php). Una colección importante de textos proveniente de \n",
    "   subtitulos de peliculas en diferentes idiomas. Muy util para entrenar traductores (incluye aliniamiento de frases \n",
    "   en diferentes idiomas), pero tambien incluye una fuente importante de frases en español. En particular, en \n",
    "   [esta página](http://opus.nlpl.eu/download.php?f=OpenSubtitles/en-es.txt.zip) se ecuentra un *corpus* con más de 500,000 frases en \n",
    "   español.\n",
    "   \n",
    "5. [**Conjunto de datos de TASS**](http://www.sepln.org/workshops/tass/2017/#datasets). Conjunto de datos de la \n",
    "   competencia organizada cada año en el *Taller de Análisis Semántico de la SEPLN (Sociedad Española de PLN)*. \n",
    "   A diferencia de los congresos en inglés, en los cuales los conjuntos de datos se abren a todo el público para\n",
    "   servir de base de pruebas sin restricciones, la SEPLN solicita una petición expresa con un documento firmado \n",
    "   sobre el uso de los datos (y el compromiso de citar a los autores de la base de datos en cada artículo). \n",
    "   Cabe aclarar que los datos son gratuitos. Si pienasan utilizar los datos para un artículo, lo conveniente es\n",
    "   solicitar los datos explicitamente a la SEPLN (en el link viene toda la información). Si los quieres utilizar \n",
    "   para probar algunas cosas, el conjunto de datos del 2015 se encuentran (por el momento) en una página personal \n",
    "   que subieron a *github* y desde ahí se pueden descargar. Se puede descargar aquí [los datos para entrenamiento \n",
    "   en general](https://raw.githubusercontent.com/imendibo/SEPLN-TASS15/master/DATA/general-tweets-train-tagged.xml), \n",
    "   y los [twits de prueba (sin etiquetar)](https://raw.githubusercontent.com/imendibo/SEPLN-TASS15/master/DATA/general-tweets-test.xml). \n",
    "   Igualmente, hay conjunto de datos para el análisis de sentimientos por aspectos, con [conjunto de entrenamiento](https://raw.githubusercontent.com/imendibo/SEPLN-TASS15/master/DATA/socialtv-tweets-train-tagged.xml) y \n",
    "   [conjunto de prueba](https://raw.githubusercontent.com/imendibo/SEPLN-TASS15/master/DATA/socialtv-tweets-test.xml).\n",
    "   Por último, para análisis de sentimientos por aspecto y entidad ([conjunto de entrenamiento](https://raw.githubusercontent.com/imendibo/SEPLN-TASS15/master/DATA/stompol-tweets-train-tagged.xml), \n",
    "   [conjunto de prueba](https://raw.githubusercontent.com/imendibo/SEPLN-TASS15/master/DATA/stompol-tweets-train-tagged.xml)). \n",
    "   Estos últimos son muy valiosos debido a que han sido anotados. \n",
    "\n",
    "\n",
    "## 1.2. Procesamiento de los datos provenientes de *wikipedia*\n",
    "\n",
    "Los datos provenientes de *wikipedia* vienen en formato `xml`, y el texto viene bajo un formato de lenguaje de marcado específico de wikipedia (muy similar a *markdown*). Es por eso que es muy importante extraer correctamente la información de los textos.\n",
    "\n",
    "Supongamos que se utilizó la [herramienta de exportación de wikipedia](https://es.wikipedia.org/wiki/Especial:Exportar) en la categoría `Poetas en español` y el documento que se obtiene lo renombramos y guardamos en `datos/wikipedia-poetas-en-espanol.xml`. Una forma sencilla de limpiar el texto, es extraer los contenidos a traves de un generador en `python`. Vamos a crear un objeto iterador que nos permita ir generando los textos correspondientes conforme se requieren. Existen diferentes métodos para limpiar texto de *wikipedia*, algunos más extremos que otros, así que vamos a proponer 3, segun el grado de limpieza inicial que busquemos en los datos.\n",
    "\n",
    "Este método no es el más elegante, pero nos permite procesar archivos xml de gran tamaño, sin tener que cargarlos completos en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et # Para manejar buffers con estructura xml\n",
    "import codecs                       # Manejo de codificacion del texto\n",
    "import re                           # Expresiones regulares\n",
    "from bs4 import BeautifulSoup       # Tratamiento de etiquetas html\n",
    "\n",
    "def generador_wiki_xml(archivo, metodo_limpieza='nada'):\n",
    "    for event, elem in et.iterparse(archivo, events=('start', 'end', 'start-ns', 'end-ns')):\n",
    "        if event == 'end':\n",
    "            if elem.tag == '{http://www.mediawiki.org/xml/export-0.10/}page':\n",
    "                elemento = {}\n",
    "                for p in elem:\n",
    "                    if p.tag == \"{http://www.mediawiki.org/xml/export-0.10/}title\":\n",
    "                        elemento['titulo'] = p.text\n",
    "                    elif p.tag == \"{http://www.mediawiki.org/xml/export-0.10/}id\":\n",
    "                        elemento['id'] = int(p.text)\n",
    "                    if p.tag == \"{http://www.mediawiki.org/xml/export-0.10/}revision\":\n",
    "                        for x in p:\n",
    "                            if x.tag == \"{http://www.mediawiki.org/xml/export-0.10/}text\": \n",
    "                                elemento['contenido'] = x.text\n",
    "                if elemento['contenido'] is not None:\n",
    "                    elemento['contenido'] = limpia_contenido(elemento['contenido'], metodo_limpieza)\n",
    "                    yield elemento\n",
    "\n",
    "    \n",
    "def limpia_contenido(contenido, metodo_limpieza='nada'):\n",
    "    if metodo_limpieza is 'nada':\n",
    "        return contenido\n",
    "    else:\n",
    "        # Elimia algunas secciones no muy informativas de la wikipedia para PLN\n",
    "        contenido = contenido[:contenido.find('== Véase también ==')]\n",
    "        contenido = contenido[:contenido.find('== Referencias ==')]\n",
    "        contenido = contenido[:contenido.find('== Notas y referencias ==')]\n",
    "        contenido = contenido[:contenido.find('== Bibliografía ==')]\n",
    "        contenido = contenido[:contenido.find('== Enlaces externos ==')]\n",
    "        contenido = contenido[:contenido.find('== Obras ==')]\n",
    "        contenido = contenido[:contenido.find('== Premios y distinciones ==')]\n",
    "        contenido = contenido[:contenido.find('==Véase también==')]\n",
    "        contenido = contenido[:contenido.find('==Referencias==')]\n",
    "        contenido = contenido[:contenido.find('==Notas y referencias==')]\n",
    "        contenido = contenido[:contenido.find('==Bibliografía==')]\n",
    "        contenido = contenido[:contenido.find('==Enlaces externos==')]\n",
    "        contenido = contenido[:contenido.find('==Obras==')]\n",
    "        contenido = contenido[:contenido.find('==Premios y distinciones==')]\n",
    "\n",
    "        contenido = contenido.lower()\n",
    "        \n",
    "        # Elimina las inserciones de xml, html y el marcado propio de wikipedia\n",
    "        contenido = re.sub(r\"{{[^(}})]*}}\",\"\", contenido)\n",
    "        contenido = re.sub(r\"\\[\\[[^\\|\\]]+\\|\", r\"[[\", contenido)\n",
    "        contenido = re.sub(r\"\\[\\[Archivo:[^(\\]\\])]*\\]\\]\",\" \", contenido)\n",
    "        contenido = re.sub(r\"\\[\\[File:[^(\\]\\])]*\\]\\]\",\" \", contenido)\n",
    "        contenido = re.sub(r\"\\[\\[Imagen:[^(\\]\\])]*\\]\\]\",\" \", contenido)\n",
    "        contenido = re.sub(r\"\\[\\[Image:[^(\\]\\])]*\\]\\]\",\" \", contenido)\n",
    "        contenido = re.sub(r\"\\[\\[Categoría:[^(\\]\\])]*\\]\\]\",\" \", contenido)\n",
    "        contenido = BeautifulSoup(contenido, \"lxml\").get_text()\n",
    "\n",
    "    if metodo_limpieza is 'maso':\n",
    "        contenido = re.sub(r\"<img([\\w\\W]+?)/?>\", \"\", contenido)\n",
    "        contenido = re.sub(r\"\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*\", r\"#URL\", contenido)\n",
    "        contenido = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", r\"#EMAIL\", contenido)\n",
    "        contenido = re.sub(r\"\\[\\[wikt[^|]*|\", \"\", contenido)\n",
    "        contenido = re.sub(r\"<ref[^<]*<\\/ref>\",\"\", contenido)\n",
    "        contenido = re.sub(r\"<.*>\",\"\", contenido)\n",
    "        contenido = re.sub(r\"&amp;\", r\"@\", contenido)\n",
    "        contenido = re.sub(r\"&lt;\", r\"<\", contenido)            \n",
    "        contenido = re.sub(r\"&gt;\", r\">\", contenido)\n",
    "        contenido = re.sub(r\"&nbsp\",\"\", contenido)\n",
    "        contenido = re.sub(r\"\\[\\[\",\"\", contenido)\n",
    "        contenido = re.sub(r\"\\]\\]\",\"\", contenido)\n",
    "        contenido = re.sub(r\"'''\",r'\"', contenido)\n",
    "        contenido = re.sub(r\"''\",r'\"', contenido)\n",
    "        contenido = re.sub(r\"={3}.*={3}\", \"\", contenido)\n",
    "        contenido = re.sub(r\"={2}.*={2}\", \"\", contenido)\n",
    "        contenido = re.sub(r\"\",r\"\", contenido)            \n",
    "        contenido = re.sub(r\"{{[^}}]*}}\",\"\", contenido)\n",
    "        contenido = contenido.replace(u'\\xa0', u' ')\n",
    "    return contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y ahora vamos a probar con el archivo `datos/wikipedia-poetas.xml` para extraer los primeros textos. Es de resaltar que no compilamos las expresiones regulares, por lo que podría ser más lento que compilarlas cada una para su ejecución (en el modulo utils_docs.py viene compilado para poder utilizar las funciones en otras libretas). Y por supuesto, se puede generar un `Data Frame` de pandas y guardar el resultado en un archivo tipo `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import randint\n",
    "archivo_xml = 'datos/wikipedia-poetas.xml'\n",
    "archivo_pkl = 'datos/wikipedia-poetas.pkl'\n",
    "\n",
    "df = pd.DataFrame.from_dict(generador_wiki_xml(archivo_xml, 'maso'))\n",
    "pd.to_pickle(df, archivo_pkl)\n",
    "\n",
    "display(df.head(10))\n",
    "i = randint(0, df.shape[0]-1)\n",
    "print(\"El contenido formateado del documento {} es el siguiente:\".format(i))\n",
    "print(df.loc[i, 'contenido'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y hagamos lo mismo con un compendio (de Wikipedia) proveniente de la categoría *Políticos de Argentina*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_xml = 'datos/wikipedia-politicos-argentina.xml'\n",
    "archivo_pkl = 'datos/wikipedia-politicos-argentina.pkl'\n",
    "\n",
    "df = pd.DataFrame.from_dict(generador_wiki_xml(archivo_xml, 'maso'))\n",
    "pd.to_pickle(df, archivo_pkl)\n",
    "\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "print(\"El contenido formateado del documento 394 es el siguiente:\")\n",
    "print(df.loc[394, 'contenido'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Obtención de textos provenientes del *Proyecto Gutenberg*\n",
    "\n",
    "El [Proyecto Gutenberg](https://www.gutenberg.org/wiki/ES_Portada) es una iniciativa para digitalizar y poner a disposición una gran cantidad de textos en diferentes idiomas, y por lo mismo es una fuente importante de datos para el procesamiento de lenguaje natural. Su principal debilidad es que solamente puede publicar libros sin derechos de autor, y por lo tanto son libros que normalmente suelen tener más de 100 años de su primera edición.\n",
    "\n",
    "En esta sección vamos a mostrar como descargar libros desde *python*. Para seleccionar el libro, es necesario [navegar en el proyecto](https://www.gutenberg.org/browse/languages/es) para seleccionar las direcciones de los libros deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import re\n",
    "\n",
    "def recupera_pg(direccion, archivo):\n",
    "    respuesta = request.urlopen(direccion)\n",
    "    libro = respuesta.read().decode('utf8')  \n",
    "    ind = libro.find(\"START OF THIS PROJECT GUTENBERG\")\n",
    "    libro = libro[ind:]\n",
    "    ind = libro.find('\\n')\n",
    "    libro = libro[ind + 1:]\n",
    "    ind = libro.find(\"*** END OF THIS PROJECT GUTENBERG\")\n",
    "    libro = libro[:ind]\n",
    "    with open(archivo, 'w', encoding='utf8') as fp:\n",
    "        fp.write(libro)\n",
    "    return True\n",
    "\n",
    "# La dirección de la obra \"El Quijote\" \n",
    "url = \"http://www.gutenberg.org/cache/epub/2000/pg2000.txt\"\n",
    "archivo = \"datos/quijote.txt\"\n",
    "recupera_pg(url, archivo)\n",
    "\n",
    "# La dirección de \"El Gaucho Martín Fierro\"\n",
    "url = \"http://www.gutenberg.org/cache/epub/14765/pg14765.txt\"\n",
    "archivo = \"datos/martin_fierro.txt\"\n",
    "recupera_pg(url, archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Obtención de datos provenientes de *twitter*\n",
    "\n",
    "Tweeter es posblemente la red social que mejor se presta para realizar PLN, y a partir de la cual se han creado varios *corpus* de prueba. En general, llos métodos para extracción de información en redes sociales funcionan más o menos igual, y por lo tanto ejemplificar el uso en *twitter* puede servir de idea de base para otras redes sociales. \n",
    "\n",
    "Para poder utilizar la interface de Twetter es necesario:\n",
    "\n",
    "1. Contar con cuenta de *Twitter* y tener registrado el telefono (para la autentificación de dos tiempos).\n",
    "\n",
    "2. Ir a la [página de desarrolladores](https://apps.twitter.com) (estando registrado en *Twetter*) y crear una nueva *App*. \n",
    "\n",
    "3. En la página generada de tu *App*, puedes recuperar en el apartado *Keys and Access Tokens* la información necesaria para poder recuperar información.\n",
    "\n",
    "4. En esta misma página debes de crear tu *Access Token* para que tu *App* pueda acceder a la información (Una clave por desarrollador y una clave por aplicación).\n",
    "\n",
    "La información la puedes recuperar fuera de linea, o en linea. Vamos a presentar las dos opciones.\n",
    "\n",
    "Para recuperar la información fuera de linea se utiliza la API más sencilla. En el ejemplo siguiente, deberías tener el texto y el autor de los últimos 100 *tweets* que recibiste en tu cuenta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = 'PN2ri0UvsLYawitO8Hako1PvM'\n",
    "consumer_secret = 'fud3LXwVza6PQMAIRmA9Y9mJ6Gc2Y1uNkV3f8G2okPZHXrCdTK'\n",
    "access_token = '43091680-zO6sw8j0VxjN2df1L3fUqMCFoRr78iNfq4eNtirqh'\n",
    "access_token_secret = 'dt3eeJgi6fzFHzalwrH7oq2B624THUTEjhiqzrqpL9BzH'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "boca_tweets = api.user_timeline('@BocaJrsOficial', count=10)\n",
    "river_tweets = api.user_timeline('@CARPoficial', count=10)\n",
    "\n",
    "for tweet in boca_tweets + river_tweets:\n",
    "    print(tweet.author.name + ': ')\n",
    "    print(tweet.text)\n",
    "    print(20*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien se pueden buscar los trending topics (globales, por país o por zona). Esto es útil si queremos hacer una base de datos con la evolución de los temas en el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'PN2ri0UvsLYawitO8Hako1PvM'\n",
    "consumer_secret = 'fud3LXwVza6PQMAIRmA9Y9mJ6Gc2Y1uNkV3f8G2okPZHXrCdTK'\n",
    "access_token = '43091680-zO6sw8j0VxjN2df1L3fUqMCFoRr78iNfq4eNtirqh'\n",
    "access_token_secret = 'dt3eeJgi6fzFHzalwrH7oq2B624THUTEjhiqzrqpL9BzH'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "mx_trends = api.trends_place(23424900)[0]\n",
    "ar_trends = api.trends_place(23424747)[0]\n",
    "\n",
    "print(\"\\n\\n********* Trends en México ***********\")\n",
    "for trend in mx_trends['trends'][:10]:\n",
    "    print(10*'=')\n",
    "    print('Titulo: ' + trend['name'])\n",
    "    print('URL: ' + trend['url'])\n",
    "\n",
    "print(\"\\n\\n********* Trends en Argentina ***********\")\n",
    "for trend in ar_trends['trends'][:10]:\n",
    "    print(10*'=')\n",
    "    print('Titulo: ' + trend['name'])\n",
    "    print('URL: ' + trend['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos recuperar (y posiblemente procesar) *tweets* en linea  tenemos que hacer algo un poco más complejo. Este script hay que dejarlo ejecutar por un tiempo y luego hay que detener la ejecución de la celda (botón de paro en la barra de herramientas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "\n",
    "consumer_key = 'PN2ri0UvsLYawitO8Hako1PvM'\n",
    "consumer_secret = 'fud3LXwVza6PQMAIRmA9Y9mJ6Gc2Y1uNkV3f8G2okPZHXrCdTK'\n",
    "access_token = '43091680-zO6sw8j0VxjN2df1L3fUqMCFoRr78iNfq4eNtirqh'\n",
    "access_token_secret = 'dt3eeJgi6fzFHzalwrH7oq2B624THUTEjhiqzrqpL9BzH'\n",
    "\n",
    "archivo = 'datos/tweets_ejemplo.csv'\n",
    "\n",
    "class Listener(tweepy.streaming.StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            tweet = json.loads(data)\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "            return True\n",
    "\n",
    "        texto = tweet['text'].replace('\\n', ' ').replace('\\t', ' ')\n",
    "        usuario = '@' + tweet.get('user').get('screen_name')\n",
    "        fecha = tweet.get('created_at')\n",
    "        tweet = u'{}\\t{}\\t{}\\n'.format(texto, usuario, fecha)\n",
    "        \n",
    "        self.archivo.write(tweet)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "with  open(archivo, 'a', encoding='utf-8') as fp:\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    \n",
    "    listner = Listener()\n",
    "    listner.archivo = fp\n",
    "    twitterStream = tweepy.Stream(auth, listner)\n",
    "    \n",
    "    twitterStream.filter(track=['AMLO', 'MORENA', 'Lopez Obrador'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada vez que volvamos a ejecutar el script anetrios aumentará el *corpus* que estamos creando.\n",
    "\n",
    "Para el filtro se pueden utilizar diferentes criterios:\n",
    "\n",
    "- track: Una lista con frases o palabras que deben encontrarse en el texto del tweet\n",
    "- folow: Una lista con IDs de usuarios a esperar sus *tweets*\n",
    "- locations: Una lista con cuatro valores delimitando la región donde se buscarán tweets\n",
    "\n",
    "El archivo que generamos, lo podemos leer ahora como un `DataTable` de *pandas*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "archivo = 'datos/tweets_ejemplo.csv'\n",
    "dt = pd.read_csv(archivo, sep='\\t', header=None, names=['Texto', 'Usuario', 'Creado el'])\n",
    "dt.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Procesamiento de los datos TASS 2015\n",
    "\n",
    "Con el fin de completar el proceso de tratamiento de información, vamos a generar un `DataFrame` en *pandas* con el fin de utilizarlo posteriormente en las tareas de PLN subsecuentes del conjunto de datos más sencillo (únicamente polaridad general de cada *tweet*. Los archivos vienen en formato *xml* por lo que su procesamiento es relativamente simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "import pandas as pd\n",
    "\n",
    "archivo_train = \"datos/tass2015/general-tweets-train-tagged.xml\"\n",
    "pickle_train = 'datos/tass2015/general-tweets-train-dt.pkl'\n",
    "archivo_test = \"datos/tass2015/general-tweets-test.xml\"\n",
    "pickle_test = 'datos/tass2015/general-tweets-test-dt.pkl'\n",
    "\n",
    "arbol = et.parse(archivo_train)\n",
    "raiz = arbol.getroot()\n",
    "data_dic = []\n",
    "for tweet in raiz.iter('tweet'):\n",
    "    contenido = tweet.find('content').text\n",
    "    if contenido is not None:\n",
    "        data_dic.append({\n",
    "            'texto': contenido,\n",
    "            'polaridad': tweet.find('sentiments')[0].find('value').text,\n",
    "            'id': tweet.find('tweetid').text,\n",
    "            'usuario': '@' + tweet.find('user').text,\n",
    "            'fecha': tweet.find('date').text,\n",
    "            'tópicos': '[' + ', '.join(['\"' + t.text + '\"' for t in tweet.find('topics')]) + ']'\n",
    "        })\n",
    "df_train = pd.DataFrame.from_dict(data_dic)\n",
    "display(df_train.head(10))\n",
    "pd.to_pickle(df_train, pickle_train)\n",
    "\n",
    "arbol = et.parse(archivo_test)\n",
    "raiz = arbol.getroot()\n",
    "data_dic = []\n",
    "for tweet in raiz.iter('tweet'):\n",
    "    contenido = tweet.find('content').text\n",
    "    if contenido is not None:\n",
    "        data_dic.append({\n",
    "            'texto': contenido,\n",
    "            'id': tweet.find('tweetid').text,\n",
    "            'usuario': '@' + tweet.find('user').text,\n",
    "            'fecha': tweet.find('date').text,\n",
    "        })\n",
    "df_test = pd.DataFrame.from_dict(data_dic)\n",
    "display(df_test.head(10))\n",
    "pd.to_pickle(df_train, pickle_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
