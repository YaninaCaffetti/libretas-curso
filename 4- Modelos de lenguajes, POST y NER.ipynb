{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/cabecera.png\" width=\"900\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelos de lenguajes (libreta de curso)\n",
    "\n",
    "## Curso Procesamiento de Lenguaje Natural \n",
    "\n",
    "### Maestría en Tecnologías de la información\n",
    "\n",
    "\n",
    "\n",
    "#### Julio Waissman Vilanova (julio.waissman@unison.mx)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a repasar los conceptos para la creación y análisis de modelos probabilísticos de secuencias de palabras (o *tokens*). Una exceente explicación sobre este tema se puede encontrar en [este capítulo del libro SLP](https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Modelos de lenguajes por $n$-gramas\n",
    "\n",
    "\n",
    "Vamos a desarrollar paso a paso uno de los modelos probabilísticos de lenguaje: el modelo de trigramas, asumiendo la hipótesis que la probabilidad que en un texto aparezca una palabra, depende *únicamente* de las dos palabras anteriores. Esto és, la probabilidad de la existencia de un trigrama específico en una posición dada, conociendo el bigrama de las dos primeras palabras del trigrama. \n",
    "\n",
    "El modelo más simple se entrena básicamente contando trigramas y contando bigramas. Para esto vamos a utilizar dos estructuras de datos que provée *python*: `Counter` y `defaultdict`. `Counter` genera una estructura que hereda de `dic` y que permite contar el número de ocurrencias de cada elemento en una secuencia (o iterable, como se dice en el argot *pythonero*).\n",
    "\n",
    "Vamos a encontrar y numerar todos los trigramas de la obra *Martín Fierro el Gaucho*, y para eso vamos a separar la obra en frases y cada frase en palabras (dos *tokenizadores*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la linea 220 se encuentra la frase: \n",
      "\n",
      "si salen a perseguir\n",
      "después de mucho aparato,\n",
      "tuitos se pelan al rato\n",
      "y va quedando el tendal:\n",
      "esto es como en un nidal\n",
      "echarle güevos a un gato.\n",
      "\n",
      "\n",
      "Separada en tokens es: ['si', 'salen', 'a', 'perseguir', 'después', 'de', 'mucho', 'aparato', 'tuitos', 'se', 'pelan', 'al', 'rato', 'y', 'va', 'quedando', 'el', 'tendal', 'esto', 'es', 'como', 'en', 'un', 'nidal', 'echarle', 'güevos', 'a', 'un', 'gato']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from random import randint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "\n",
    "archivo = \"datos/martin_fierro.txt\"\n",
    "\n",
    "with open(archivo, 'r', encoding='utf8') as fp:\n",
    "    texto_crudo = fp.read()\n",
    "\n",
    "# Definimos el tokenizador en frases\n",
    "# ¿Que pasaría si usamos palabras en minúscula y mayuscula?\n",
    "# ¿Y si no eliminamos los signos de puntuación?\n",
    "def tokenizador_frases(texto):\n",
    "    # Minúsculas\n",
    "    texto_preparado = texto.lower()\n",
    "    # Eliminamos números\n",
    "    texto_preparado = re.sub(r'[0-9]+', '', texto_preparado)\n",
    "    # Cambiamos 'Ud.' por 'usted'\n",
    "    texto_preparado = re.sub(r'Ud\\.', 'usted', texto_preparado)\n",
    "    # Separamos el texto en sentencias\n",
    "    frases = sent_tokenize(texto_preparado, language='spanish')\n",
    "    return frases\n",
    "\n",
    "# definimos el tokenizador de palabras\n",
    "# tokenizador_palabras = lambda sentencia: word_tokenize(sentencia, language='spanish')\n",
    "tokenizador_palabras = lambda sentencia: regexp_tokenize(sentencia, r'\\w+')\n",
    "\n",
    "#Ejemplo\n",
    "i = randint(0, 300)\n",
    "print(\"En la linea {} se encuentra la frase: \\n\\n{}\".format(i, tokenizador_frases(texto_crudo)[i]))\n",
    "print(\"\\n\\nSeparada en tokens es: {}\".format(tokenizador_palabras(tokenizador_frases(texto_crudo)[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora, vamos a definir un generador, el cual vamos a usar para calcular cuantas veces aparecen cada uno de los trigramas, utilizando la función `trigrams` del módulo *nltk*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los trigramas que más aparecen son:\n",
      "\t<s> <s> y           \t(67 veces)\n",
      "\t<s> <s> yo          \t(26 veces)\n",
      "\t<s> <s> no          \t(19 veces)\n",
      "\t<s> <s> si          \t(15 veces)\n",
      "\t<s> <s> el          \t(13 veces)\n",
      "\t<s> <s> a           \t(12 veces)\n",
      "\t<s> <s> pero        \t(12 veces)\n",
      "\tlo mesmo que        \t(11 veces)\n",
      "\t<s> <s> me          \t(11 veces)\n",
      "\tahi no más          \t(9 veces)\n",
      "\t<s> <s> en          \t(9 veces)\n",
      "\t<s> <s> por         \t(8 veces)\n",
      "\t<s> <s> que         \t(8 veces)\n",
      "\t<s> <s> al          \t(8 veces)\n",
      "\t<s> <s> cuando      \t(8 veces)\n",
      "\tque el hombre       \t(8 veces)\n",
      "\t<s> <s> se          \t(7 veces)\n",
      "\t<s> <s> ah          \t(7 veces)\n",
      "\t<s> <s> era         \t(7 veces)\n",
      "\t<s> <s> de          \t(7 veces)\n"
     ]
    }
   ],
   "source": [
    "from nltk import trigrams\n",
    "from collections import Counter\n",
    "\n",
    "tri_generador = (\n",
    "    trigrama for frase in tokenizador_frases(texto_crudo)\n",
    "        for trigrama in trigrams(\n",
    "            tokenizador_palabras(frase), \n",
    "            pad_left = True, left_pad_symbol=\"<s>\",\n",
    "            pad_right = True, right_pad_symbol='</s>'\n",
    "        )\n",
    ")    \n",
    "tri_cuentas = Counter(tri_generador)\n",
    "\n",
    "trigramas = list(tri_cuentas.keys())\n",
    "ejemplo = trigramas[randint(0, len(trigramas) - 1)]\n",
    "\n",
    "print(\"Los trigramas que más aparecen son:\")\n",
    "for (trig, cuenta) in tri_cuentas.most_common(20):\n",
    "    print(\"\\t{:20}\\t({} veces)\".format(' '.join(trig), cuenta))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para obtener un modelo, pues simplemente dividimos el número de veces que aparece cada trigrama, entre el número de veces que aparece el bigrama conformado con las dos primeras palabras del trigrama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13480885311871227\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "from collections import defaultdict\n",
    "\n",
    "bi_generador = (\n",
    "    bigrama for frase in tokenizador_frases(texto_crudo) \n",
    "        for bigrama in bigrams(\n",
    "            [\"<s>\"] + tokenizador_palabras(frase), \n",
    "            pad_left = True, left_pad_symbol=\"<s>\",\n",
    "            pad_right = True, right_pad_symbol='</s>'\n",
    "        )\n",
    ")    \n",
    "bi_cuentas = Counter(bi_generador)\n",
    "\n",
    "modelo = defaultdict(lambda: 0)  #  Si el trigrama no aparece la probabilidad es 0\n",
    "for trigrama in trigramas:\n",
    "    modelo[trigrama] = tri_cuentas[trigrama] / bi_cuentas[trigrama[:-1]]\n",
    "    \n",
    "print(modelo[('<s>', '<s>', 'y')])\n",
    "print(modelo[('y', 'ansí', 'estaba')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos en este caso, si un trigrama no aparece *exactamente* en el texto, el modelo asume una probabilidad 0. Estos es muy peligroso, ya que, debido a la hipótesis de independencia condicional que hicimos, la probabilidad de una frase es la multiplicación de la probabilidad de sus trigramas. Este problema lo resolveremos más adelante.\n",
    "\n",
    "Por el momento veamos que tipos de frases se generan a partir de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def generador_sentencia(modelo):\n",
    "    trigramas = modelo.keys()\n",
    "    cadena = ['<s>','<s>']\n",
    "    while cadena[-1] != '</s>':\n",
    "        w1_w2 = (cadena[-2], cadena[-1])\n",
    "        rdn = random()\n",
    "        for t in trigramas:\n",
    "            if t[:-1] == w1_w2:\n",
    "                rdn -= modelo[t]\n",
    "            if rdn <= 0:\n",
    "                cadena.append(t[-1])\n",
    "                break\n",
    "    return ' '.join(cadena[2:-1])\n",
    "\n",
    "for i in range(5):\n",
    "    print(generador_sentencia(modelo))\n",
    "    print(30*'-')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que el *corpus* es relativamente pequeño algunas frases generadas son exactamente iguales a las del texto original. Sin embargo la gran mayoría son diferentes. Si entrenamos con un corpus más grande (como las obras completas de Cervantes), las frases son claramente diferentes a las originales. \n",
    "\n",
    "Ahora veamos si podemos calcular la perplejidad de un documento respecto al modelo probabilístico. La perplejidad de un documento $d$ respecto al modelo de trigramas $m$ se calcula como:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{P}_m(d) &= p_m(d)^{1/N},\\\\ \n",
    "p_m(d) &= \\prod_1^{N+1}p_m(w_i|w_{i-1}, w_{i-2})\n",
    "\\end{align}\n",
    "\n",
    "donde $p_m(p_m(w_i|w_{i-1}, w_{i-2})$ es la probabilidad condicional del trigama $w_{i-2}, w_{i-1}, w_{i}$ que acabamos de calcular en el modelo y $N$ es el número de tokens del texto a evaluar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplejidad(texto, modelo):\n",
    "    # Generador\n",
    "    tri_generador = (\n",
    "        trigrama for frase in tokenizador_frases(texto)\n",
    "            for trigrama in trigrams(\n",
    "                tokenizador_palabras(frase), \n",
    "                pad_left = True, left_pad_symbol=\"<s>\",\n",
    "                pad_right = True, right_pad_symbol='</s>'\n",
    "            )\n",
    "    )\n",
    "    N = 0\n",
    "    p_sentencia = 1\n",
    "    for trigrama in tri_generador:\n",
    "        p_sentencia *= modelo[trigrama]\n",
    "        N += 1\n",
    "    if p_sentencia == 0:\n",
    "        print(\"Cuidado! La probabilidad de la frase es 0 y la perplejidad infinita\")\n",
    "        return None\n",
    "    return (1/p_sentencia)**(1/N)\n",
    "\n",
    "frase_1 = \"Y ya me había hallao más prendido que un botón.\"\n",
    "frase_2 = \"Y ya me había hallao más prendido que un baile.\"\n",
    "\n",
    "print(\"La frase <<{}>> tiene una perplejidad de {}\\n\".format(frase_1, perplejidad(frase_1, modelo)))\n",
    "print(\"La frase <<{}>> tiene una perplejidad de {}\".format(frase_2, perplejidad(frase_2, modelo)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, inclusive en frases que utilizan el mismo vocabulario del texto, la perplejidad se vuelve infinita. Es por esa razón que es necesario *suavizar* el modelo, asumiendo que existe un grado de ignorancia estadística en nuestra evidencia (el *corpus*). El método más usado en PLN de suavizado es el llamado *Kneser Ney*, el cual es laborioso de implementar, una explicación a grandes razgos del método la puedes encontrar [aqui](http://www.foldl.me/2014/kneser-ney-smoothing/).\n",
    "\n",
    "Afortunadamente, en *nltk* ya existe la distribución discreta con suavizado de Kneser Ney*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from math import pow\n",
    "from nltk import FreqDist, KneserNeyProbDist\n",
    "\n",
    "def modelado_kn(corpus, tokenizador_palabras, tokenizador_frases):\n",
    "    return KneserNeyProbDist(\n",
    "        FreqDist(generador_trigramas(corpus, tokenizador_palabras, tokenizador_frases))\n",
    "    )\n",
    "\n",
    "def generador_trigramas(texto, tokenizador_palabras, tokenizador_frases):\n",
    "    tri_generador = (\n",
    "        trigrama for frase in tokenizador_frases(texto) \n",
    "             for trigrama in trigrams(\n",
    "                tokenizador_palabras(frase), \n",
    "                pad_left = True, left_pad_symbol=\"<s>\",\n",
    "                pad_right = True, right_pad_symbol='</s>'\n",
    "        )\n",
    "    ) \n",
    "    return tri_generador        \n",
    "\n",
    "def genera_sentencia(modelo):\n",
    "    trigramas = list(modelo.samples())\n",
    "    cadena = ['<s>','<s>']\n",
    "    while cadena[-1] != '</s>':\n",
    "        w1_w2 = (cadena[-2], cadena[-1])\n",
    "        rdn = random()\n",
    "        for trigrama in trigramas:\n",
    "            if trigrama[:-1] == w1_w2:\n",
    "                rdn -= modelo.prob(trigrama)\n",
    "            if rdn <= 0:\n",
    "                cadena.append(trigrama[-1])\n",
    "                break\n",
    "    return ' '.join(cadena[2:-1])\n",
    "\n",
    "def perplejidad(texto, modelo):\n",
    "    vocab = {t[2] for t in modelo.samples()}\n",
    "    N = 0\n",
    "    p = 1\n",
    "    for trigrama in generador_trigramas(texto, tokenizador_palabras, tokenizador_frases):\n",
    "        if trigrama[-1] not in vocab:\n",
    "            print(\"Cuidado! La probabilidad de la frase es 0 y la perplejidad infinita\")\n",
    "            return None            \n",
    "        lista_trigramas = [t for t in modelo.samples() if t[:-1] == trigrama[:-1]]\n",
    "        if not lista_trigramas:\n",
    "            p *= 1/len(vocab)\n",
    "        if trigrama in lista_trigramas:\n",
    "            p *= modelo.prob(trigrama)\n",
    "        else:\n",
    "            no_p = sum(modelo.prob(t) for t in lista_trigramas)\n",
    "            p *= (1 - no_p)\n",
    "        N += 1\n",
    "    return (1 / p) ** (1 / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a probarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_kn = modelado_kn(texto_crudo, tokenizador_palabras, tokenizador_frases)\n",
    "\n",
    "for _ in range(5):\n",
    "    print(genera_sentencia(modelo_kn))\n",
    "    print(30 * '-')\n",
    " \n",
    "frase_1 = \"Y ya me había hallao más prendido que un botón.\"\n",
    "frase_2 = \"Y ya me había hallao más prendido que un baile.\"\n",
    "frase_3 = 'Y ya me habia hallao más cosida que un botón.'\n",
    "frase_4 = \"Y aquí estoy poniendo una frase que seguro no es del libro para saber su perplejidad.\"\n",
    "\n",
    "p_str = \"La frase <<{}>> tiene una perplejidad de {}\\n\"\n",
    "print(p_str.format(frase_1, perplejidad(frase_1, modelo_kn)))\n",
    "print(p_str.format(frase_2, perplejidad(frase_2, modelo_kn)))\n",
    "print(p_str.format(frase_3, perplejidad(frase_3, modelo_kn)))\n",
    "print(p_str.format(frase_4, perplejidad(frase_4, modelo_kn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, en este modelo, si no encuentra un bigrama existente, tambien calcula una perplejidad infinita a una frase que no estaría muy lejana. \n",
    "\n",
    "Para resolver esto, es necesario utilizar el concepto de *palabras fuera de vocabulario (OOV)* y esto se logra con un diccionario preestablecido, o generando nuevos tokens en el texto. Vamos a generar un nuebo modelo que utilice palabras OOV (marcadas como '<UNK>'). Vamos a construir el vocabulario, y cada vez que se encuentre por primera vez una palabra la vamos a substituir por <UNK>. Si aparece una segunda vez, ya queda en su forma original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelado_knOOV(corpus, tokenizador_palabras, tokenizador_frases):\n",
    "    vocabulario = set([])\n",
    "    modelo = KneserNeyProbDist(\n",
    "        FreqDist(generador_trigramasOOV(corpus, tokenizador_palabras, tokenizador_frases, vocabulario))\n",
    "    )\n",
    "    return modelo, vocabulario\n",
    "\n",
    "def generador_trigramasOOV(texto, tokenizador_palabras, tokenizador_frases, vocab):\n",
    "    vocab_ini = set([])\n",
    "    if len(vocab) == 0:\n",
    "        def token(frase):\n",
    "            for t in tokenizador_palabras(frase):\n",
    "                if t not in vocab_ini:\n",
    "                    vocab_ini.add(t)\n",
    "                    yield '<UNK>'\n",
    "                else:\n",
    "                    vocab.add(t)\n",
    "                    yield t\n",
    "    else:\n",
    "        def token(frase):\n",
    "            for t in tokenizador_palabras(frase):\n",
    "                yield t if t in vocab else '<UNK>'\n",
    "    tri_generador = (\n",
    "        trigrama for frase in tokenizador_frases(texto) \n",
    "             for trigrama in trigrams(\n",
    "                token(frase), \n",
    "                pad_left = True, left_pad_symbol=\"<s>\",\n",
    "                pad_right = True, right_pad_symbol='</s>'\n",
    "        )\n",
    "    ) \n",
    "    return tri_generador    \n",
    "\n",
    "def perplejidadOOV(texto, modelo, vocab):\n",
    "    N = 0\n",
    "    p = 1\n",
    "    for trigrama in generador_trigramasOOV(texto, tokenizador_palabras, tokenizador_frases, vocab):\n",
    "        lista_trigramas = [t for t in modelo.samples() if t[:-1] == trigrama[:-1]]\n",
    "        if not lista_trigramas:\n",
    "            p *= 1/(len(vocab) + 1)\n",
    "        if trigrama in lista_trigramas:\n",
    "            p *= modelo.prob(trigrama)\n",
    "        else:\n",
    "            no_p = sum(modelo.prob(t) for t in lista_trigramas)\n",
    "            p *= (1 - no_p)/(len(vocab) - len(lista_trigramas))\n",
    "        N += 1\n",
    "    return (1 / p) ** (1 / N)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora lo aplicamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_knOOV, vocab = modelado_knOOV(texto_crudo, tokenizador_palabras, tokenizador_frases)\n",
    "\n",
    "for _ in range(5):\n",
    "    print(genera_sentencia(modelo_knOOV))\n",
    "    print(30 * '-')\n",
    "    \n",
    "frase_1 = \"Y ya me había hallao más prendido que un botón.\"\n",
    "frase_2 = \"Y ya me había hallao más prendido que un baile.\"\n",
    "frase_3 = 'Y ya me habia hallao más cosida que un botón.'\n",
    "frase_4 = \"Y aquí estoy poniendo una frase que seguro no es del libro para saber su perplejidad.\"\n",
    "\n",
    "p_str = \"La frase <<{}>> tiene una perplejidad de {}\\n\"\n",
    "print(p_str.format(frase_1, perplejidadOOV(frase_1, modelo_knOOV, vocab)))\n",
    "print(p_str.format(frase_2, perplejidadOOV(frase_2, modelo_knOOV, vocab)))\n",
    "print(p_str.format(frase_3, perplejidadOOV(frase_3, modelo_knOOV, vocab)))\n",
    "print(p_str.format(frase_4, perplejidadOOV(frase_4, modelo_knOOV, vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Marcado de *parte del discurso* (POST) y reconocimiento de entidades (NER)\n",
    "\n",
    "El marcado de *parte del discurso (POST)* así como el *reconocimiento de entidades (NER)* son más o menos la misma tarea. Es interesante armar un modelo basado en *Campos aleatorios condicionales (CRF)*, pero el problema es que para su entrenamiento se requieren muchos ejemplos de texto anotado. El texto anotado es una de las cosas más difíciles de encontrar, por lo que, para estos problemas, se suelen usar modelos pre-entrenados.\n",
    "\n",
    "En este apartado vamos a ver como utilizar *spacy* como herramienta tanto para POST como NER. Para esto, previamente hemos descargado el modelo en español de `spacy` de tamaño medio. Este modelo está entrenado bajo diferentes fuentes y en general funciona bastante bien. Para una explicación de como descargar el modelo (si lo quieres usar fuera del contenedor) y como fue entrenado, puedes consultar [aqui](https://spacy.io/models/es#section-es_core_news_md).\n",
    "\n",
    "La biblioteca de *spacy* ha cobrado en el último año mucha fuerza, sobre todo en aplicaciones reale, ya que si bien tarda en cargar el modelo, es muy rápida para el procesamiento, y su operación es muy sencilla, como veremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien, para el inglés, *spacy* mantiene [la notación POS propuesta por el proyecto *Penn Treebank*](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), en español utiliza su propia notación, que es bastante intuitiva y se puede consultar con el comando `spacy.explain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set([tag.strip('_').split('__')[0] for tag in spacy.lang.es.TAG_MAP.keys()])\n",
    "print(\"{:8}{:15}\".format(\"Tag\", \"Descripción\"))\n",
    "print(23*'-')\n",
    "for tag in tags:\n",
    "    print(\"{:8}{:15}\".format(tag, spacy.explain(tag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, vamos a generar el etiquetado POS para una frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practica cambiando el texto, por otros textos (para la visualización, de preferencia cortos)\n",
    "# texto = \"En el 2018, elegí la universidad pública y gratuita.\"\n",
    "# texto = \"MapUNaM contiene información sobre actividades de extensión y de investigación que realiza la Universidad Nacional de Misiones en distintos puntos de la provincia.\"\n",
    "texto = \"Mi nombre es Julio Waissman Vilanova, trabajo en la Universidad de Sonora y quisiera conocer las Cataratas de Iguazú\"\n",
    "print('Texto a analizar: <<{}>>'.format(texto))\n",
    "\n",
    "doc = nlp(texto)\n",
    "tokens_dic = []\n",
    "for token in doc:\n",
    "    tokens_dic.append({\n",
    "        'texto': token.text,\n",
    "        'lema': token.lemma_,\n",
    "        'Etiquetado POS simple': token.pos_,\n",
    "        'Etiquetado POS completo' : token.tag_,\n",
    "        '¿Palabra vacia?': token.is_stop,\n",
    "        '¿Fuera de vocabulario?': token.is_oov,\n",
    "        'Entidad': token.ent_type_,\n",
    "        'Notación BIO': token.ent_iob_\n",
    "    })\n",
    "doc_dt = pd.DataFrame.from_dict(tokens_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y vamos a visualizar el etiquetado de la frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(doc_dt[['texto', 'Etiquetado POS simple', 'Etiquetado POS completo', \n",
    "                '¿Palabra vacia?', '¿Fuera de vocabulario?']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, además del etiquetado POS sencillo, se hace un etiquetado completo que nos indica persona, genero, tiempo, plural o singular, etc. Igualmente, se nos avisa si esta palabra está considerada en el modelo como palabra vacía, o si no se encuentra registrada.\n",
    "\n",
    "Si ahora utilizamos la misma frase ya procesada para el reconocimiento de entidades tenemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(doc_dt[['texto', 'Entidad', 'Notación BIO']])\n",
    "\n",
    "for entidad in doc.ents:\n",
    "    print(\"La etiqueta {} significa <<{}>>\".format(entidad.label_, spacy.explain(entidad.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante que se mantiene la notación *BIO*. Como este análisis por palabras puede ser de utilidad para entender la estructura de una oración, *spacy* viene con un método de visualización de POST y de NER bastante atractivos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "dependencias = spacy.displacy.render(doc, style='dep')\n",
    "display(HTML(dependencias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades = spacy.displacy.render(doc, style='ent')\n",
    "display(HTML(entidades))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no hay etiquetado NER en una oración, a adevolver un error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
