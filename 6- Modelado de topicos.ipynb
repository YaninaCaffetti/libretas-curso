{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/cabecera.png\" width=\"900\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelado de tópicos (libreta de curso)\n",
    "\n",
    "## Curso Procesamiento de Lenguaje Natural \n",
    "\n",
    "### Maestría en Tecnologías de la información\n",
    "\n",
    "\n",
    "\n",
    "#### Julio Waissman Vilanova (julio.waissman@unison.mx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos de modelado de tópicos funcionan de una manera diferentea los vectores de palabras. Para el modelado de tópicos vamos a asumir que tenemos un conjunto de palabras $W = \\{w_1, \\ldots, w_n\\}$ las cuales se pueden indexar (de modo que $w_i$ le corresponde el índice $i$. Por otra parte tenemos una serie de documentos $D = \\{d_1, \\ldots, d_m\\}$ donde cada documento está representado por una secuencia de $n_j$ palabras $d_j = (w^1, \\ldots, w^{m_j})$.\n",
    "\n",
    "Si se asume que tenemos $k$ tópicos, donde $k$ se establece *a priori*, la idea del modelado de tópicos es encontrar dos matrices: la *matriz palabra a tópico* (WTM) $\\Phi$ de dimensión $n \\times k$ y la *matriz tópico a documento* (TDM) $\\Theta$ $k \\times m$. Las matrices se forman de tal manera que la entrada $\\phi_{i,k}$ de $\\Phi$ represente la probabilidad que la palabra $w_i$ conociendo el tópico, mientras que la entrada $\\theta_{k, j}$ representa la probabilidad que el documento $d_j$ se trate sobre el tópico $k$.\n",
    "\n",
    "Aqui lo interesante es que no establecemos los tópicos *a priori*, si no que las matrices se forman al minimizar una funcion objetivo. Dependiendo de la función objetivo la formación de tópicos es diferente, y los métodos de obtención de $\\Phi$ y $\\Theta$ (que para nosotros van a ser transparentes) también varian de forma importante. Los dos criterios más utilizados son los que dan a lugar a los métodos de [PLSA](https://papers.nips.cc/paper/1654-learning-the-similarity-of-documents-an-information-geometric-approach-to-document-retrieval-and-categorization.pdf) y una [LDA](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), los cuales vimos en términos generales en el curso.\n",
    "\n",
    "En particula $\\Phi$ no solamente provée una especie de matriz de vectores de palabras, si no que, ademas, nos permite ver como están distribuidos los tópicos. Un tópico es definido por el conjunto de palabras con una probabilidad significativa de encontrarse en un documento, si el documento trata sobre el tópico en cuestión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Modelado de tópicos con LSA y LDA utilizando `Gensim` \n",
    "\n",
    "Para realizar el modelado de tópicos es necesario contar con una bolsa de palabras (de preferencia utilizando la cuenta de las palabras y no únicamente el indicador) por lo que es necesario obtener, normalizar y limpiar el *corpus*. Es importante utilizar el método de BOW que provée *gensim* para realizar el modelado de tópicos.\n",
    "\n",
    "Vamos a procesar tópicos a partir de los datos de *wikipedia* sobre *políticos argentinos* que obtuvimos y tratamos  en la [primer libreta](./1- Obtencion de textos en espanyol.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from random import randint\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "palabras_vacias = set(stopwords.words('spanish'))\n",
    "palabras_vacias.update({'url', 'html', 'año', \n",
    "                        'aligncenter', 'colspan3',\n",
    "                        'bgcolor', 'stylebackground',\n",
    "                        'descargar', 'disponible',\n",
    "                        'págs', 'isbn', 'colspan', \n",
    "                        'align', 'styletext', 'px', 'thumb'})\n",
    "\n",
    "remplaza_por_espacios_re = re.compile(r'[\\d\\n/(){}\\[\\]\\|@,;\\.#+_-]')\n",
    "simbolos_a_eliminar_re = re.compile(r'[^\\w\\- ]')\n",
    "palabra_valida = re.compile(r'\\w{2,}') # mínimo dos letras\n",
    "\n",
    "def normaliza_texto(texto):\n",
    "    text = texto.strip().lower()\n",
    "    \n",
    "    # Elimina simbolos\n",
    "    text = re.sub(remplaza_por_espacios_re, ' ', text)\n",
    "    text = re.sub(simbolos_a_eliminar_re, '', text)\n",
    "    \n",
    "    return [token for token in text.split() \n",
    "            if (token not in palabras_vacias and \n",
    "                re.match(palabra_valida, token))]\n",
    "\n",
    "archivo_pkl = 'datos/wikipedia-politicos-argentina.pkl'\n",
    "wiki_df = pd.read_pickle(archivo_pkl)\n",
    "\n",
    "documentos = [normaliza_texto(documento) \n",
    "              for documento in wiki_df['contenido'].values]\n",
    "\n",
    "# Vamos a sacar un documento del corpus para luego usarlo\n",
    "ind_ejemplo = randint(0, len(documentos) - 1)\n",
    "ejemplo = documentos.pop(ind_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que hacer cuatro pasos importantes: extraer el diccionario a partir del corpus de entrenamiento; convertir los documentos a una representación de BOW; escoger el número de tópicos que queremos analizar; y por último entrenar nuestro modelo (ya sea PLSA, LDA u otro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora\n",
    "\n",
    "# Genera el diccionario de palabras\n",
    "diccionario = corpora.Dictionary(documentos)\n",
    "diccionario.filter_extremes(no_below=5, no_above=0.3)\n",
    " \n",
    "# Extrae las características en forma de BOW\n",
    "corpus = [diccionario.doc2bow(doc) for doc in documentos]\n",
    "\n",
    "# Número de tópicos\n",
    "n_topicos = 20\n",
    "    \n",
    "# Genera el modelo LDA\n",
    "modelo_lda = models.LdaModel(corpus=corpus, num_topics=n_topicos, id2word=diccionario)\n",
    " \n",
    "# Genera el modelo PLSA (LSI es equivalente)\n",
    "modelo_plsa = models.LsiModel(corpus=corpus, num_topics=n_topicos, id2word=diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahor podemos ver algunas cosas que nos permitan analisar nuestro corpus, en primer lugar, como se define a grandes razgos cada tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, topico) in modelo_lda.print_topics(num_topics=n_topicos, num_words=10):\n",
    "    print(10 * \"-\" + \"topico {}\".format(i) + 20 * \"-\")\n",
    "    print(topico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a ver como se clasificaría en los diferentes tópicos el documento que no utilizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ejemplo = wiki_df.loc[ind_ejemplo, 'contenido']\n",
    "print(\"El artículo \\n<<{}>>\\n\\n tiene los siguientes tópicos:\".format(texto_ejemplo))\n",
    "\n",
    "topicos_ejemplo = modelo_lda[diccionario.doc2bow(ejemplo)]\n",
    "topicos_ejemplo.sort(key=lambda x:x[1], reverse=True)\n",
    "for (topico, peso) in topicos_ejemplo:\n",
    "    print(\"\\tTopico {} con un peso de {}\".format(topico, peso))\n",
    "\n",
    "print(\"\\n\\nEl tópico con mayor peso positivo es:\")\n",
    "print(modelo_lda.print_topic(topicos_ejemplo[0][0]))\n",
    "print(30 * '-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una manera de revisar la calidad del modelo es a partir de la perplejidad de uno o varios documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pow\n",
    "\n",
    "P = modelo_lda.log_perplexity(corpus)\n",
    "pow(2, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Visualización de tópicos con `pyLDAvis`\n",
    "\n",
    "Como vimos, explorar los modelos creados y ajustarlos para tratar de extraer mejores tópicos es una tarea ingrata si no se cuenta con herramientas de visualización. El móduo `pyLDAvis` es un clon en python de la misma herramienta (`LDAvis`) que existe en *R*. La gran ventaja es que la herramienta está diseñada para jugar bien con *jupyter*.\n",
    "\n",
    "`LDAvis` funciona únicamente con modelos que proveen el método `inference`, como LDA que la estimación de tópicos se basa en una inferencia estadística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(corpus=corpus, dictionary=diccionario, topic_model=modelo_lda)\n",
    "panel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
